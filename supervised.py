# -*- coding: utf-8 -*-
"""Supervised.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q2xrb3QUNoqtv32ENfZpi2SVMEuuFYnx

# **Libraries**
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder,StandardScaler,MinMaxScaler

"""# **Loading Dataset**"""

df = pd.read_csv('csv_result-KDDTrain+_20Percent.csv')
df

"""# **Data Analysis**"""

df.describe()

df.info()

df.isnull().sum()

df = df.dropna()
df

dup = df.duplicated()
dup

df = df.drop(columns=['id','duration'])
df



"""# **Data Preprocessing**"""

cat = ['protocol_type','service','flag']
Encoder = LabelEncoder()
for i in cat:
    df[i] = Encoder.fit_transform(df[i])
df

target = df['class']
features = df.drop(columns='class')
features

num_col = features.select_dtypes(include=['int64','float64']).columns
num_col

features.hist()

scaler = MinMaxScaler()
features[num_col] = scaler.fit_transform(features[num_col])
features



df = pd.concat([features,target],axis=1)
df

df

"""# **Feature Selection**"""

from sklearn.ensemble import ExtraTreesClassifier
x = df.drop(columns='class')
y = df['class']
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)
model = ExtraTreesClassifier(n_estimators=100)
model.fit(x_train,y_train)
predict = model.predict(x_test)
predict

from sklearn.metrics import accuracy_score
score = accuracy_score(y_test,predict)*100
score

x_train

y_train

x_test

y_test

imp_features = model.feature_importances_
imp_features

feature_importance_df = pd.DataFrame({
    'Feature': x.columns,
    'Importance': imp_features
})


feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)


print("Feature Importances:")
print(feature_importance_df)


threshold = 0.03
important_features = feature_importance_df[feature_importance_df['Importance'] > threshold]
print("\nImportant Features (above threshold):")
print(important_features)

features = [
    "same_srv_rate",
    "logged_in",
    "dst_host_srv_count",
    "dst_host_srv_serror_rate",
    "dst_host_same_srv_rate",
    "serror_rate",
    "srv_serror_rate",
    "protocol_type",
    "dst_host_same_src_port_rate",
    "flag"
]
df_new = df[features]
target = df['class']
df_new

target

X = df_new
Y = target

X

Y

import matplotlib.pyplot as plt
df_new.hist()
plt.tight_layout()
plt.show()

'''import seaborn as sns
sns.pairplot(df_new)
plt.tight_layout()
plt.show()'''

import seaborn as sns
plt.figure(figsize=(15,10))
sns.heatmap(data=df_new.corr(),annot=True)
plt.tight_layout()
plt.show()



"""# **SMOTE**"""

from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from collections import Counter
X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2)
smote = SMOTE(random_state = 42)
x_train_resampled,y_train_resampled = smote.fit_resample(X_train,Y_train)

print("Original Class Distribution:",Counter(y_train))
print("Resampled Class Distribution:",Counter(y_train_resampled))

y_train_resampled.hist()

Y.hist()

y_test

x_train_resampled

y_test

y_train_resampled

"""# **Machine Learning Models**"""

from sklearn.ensemble import RandomForestClassifier
Rmodel = RandomForestClassifier(n_estimators=600,random_state=40,max_depth=15,min_samples_split= 5)
Rmodel.fit(x_train_resampled,y_train_resampled)

Rpredict = Rmodel.predict(X_test)
Rpredict

from sklearn.linear_model import LogisticRegression
Lmodel = LogisticRegression()
Lmodel.fit(x_train_resampled,y_train_resampled)

Lpredict = Lmodel.predict(X_test)
Lpredict

from sklearn.neighbors import KNeighborsClassifier
Kmodel = KNeighborsClassifier(n_neighbors=5)
Kmodel.fit(x_train_resampled,y_train_resampled)

Kpredict = Kmodel.predict(X_test)
Kpredict

from sklearn.tree import DecisionTreeClassifier
Dmodel = DecisionTreeClassifier()
Dmodel.fit(x_train_resampled,y_train_resampled)

Dpredict = Dmodel.predict(X_test)
Dpredict

from sklearn.ensemble import AdaBoostClassifier
Amodel = AdaBoostClassifier(n_estimators=2000,random_state=40)
Amodel.fit(x_train_resampled,y_train_resampled)

Apredict = Amodel.predict(X_test)
Apredict

from sklearn.svm import SVC
Smodel = SVC()
Smodel.fit(x_train_resampled,y_train_resampled)

Spredict = Smodel.predict(X_test)
Spredict

y_train_Resampled = Encoder.fit_transform(y_train_resampled)
import xgboost as xgb
Xmodel = xgb.XGBClassifier(objective='binary:logistic',use_label_encoder=False)
Xmodel.fit(x_train_resampled,y_train_Resampled)

Xpredict = Xmodel.predict(X_test)
Xpredict

X_test.shape



"""# **Performance Metrics**

# Random Forest

Accuracy score
"""

from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score
Rascore = accuracy_score(Y_test,Rpredict)*100
Rascore

"""Precision Score"""

Rpscore = precision_score(Y_test,Rpredict,average='macro')*100
Rpscore

"""F1 Score"""

Rfscore = f1_score(Y_test,Rpredict,average='macro')*100
Rfscore

"""Recall score"""

Rrecall = recall_score(Y_test,Rpredict,average='macro')*100
Rrecall

"""# Logisitic Regression"""

Lascore = accuracy_score(Y_test,Lpredict)*100
Lascore

Lpscore = precision_score(Y_test,Lpredict,average='macro')*100
Lpscore

Lfscore = f1_score(Y_test,Lpredict,average='macro')*100
Lfscore

Lrecall = recall_score(Y_test,Lpredict,average='macro')*100
Lrecall

"""# KNN"""

Kascore = accuracy_score(Y_test,Kpredict)*100
Kascore

Kpscore = precision_score(Y_test,Kpredict,average='macro')*100
Kpscore

Kfscore = f1_score(Y_test,Kpredict,average='macro')*100
Kfscore

Krecall = recall_score(Y_test,Kpredict,average='macro')*100
Krecall

"""# DECISION TREE"""

Dascore = accuracy_score(Y_test,Dpredict)*100
Dascore

Dpscore = precision_score(Y_test,Dpredict,average='macro')*100
Dpscore

Dfscore = f1_score(Y_test,Dpredict,average='macro')*100
Dfscore

Drecall = recall_score(Y_test,Dpredict,average='macro')*100
Drecall

"""# ADABOOST"""

Aascore = accuracy_score(Y_test,Apredict)*100
Aascore

Apscore = precision_score(Y_test,Apredict,average='macro')*100
Apscore

Afscore = f1_score(Y_test,Apredict,average='macro')*100
Afscore

Arecall = recall_score(Y_test,Apredict,average='macro')*100
Arecall

"""# **SVM**"""

Sscore = accuracy_score(Y_test,Spredict)*100
Sscore

Spscore = precision_score(Y_test,Spredict,average='macro')*100
Spscore

Sfscore = f1_score(Y_test,Spredict,average='macro')*100
Sfscore

Srecall = recall_score(Y_test,Spredict,average='macro')*100
Srecall

"""# XGBOOST"""

Xscore = accuracy_score(Encoder.fit_transform(Y_test),Xpredict)*100
Xscore

Xpscore = precision_score(Encoder.fit_transform(Y_test),Xpredict,average='macro')*100
Xpscore

Xfscore = f1_score(Encoder.fit_transform(Y_test),Xpredict,average='macro')*100
Xfscore

Xrecall = recall_score(Encoder.fit_transform(Y_test),Xpredict,average='macro')*100
Xrecall

import matplotlib.pyplot as plt
x_values = ['RF','LR','KNN','DT','AdaBoost','SVM','XGBoost']
y_values = [Rascore,Lascore,Kascore,Dascore,Aascore,Sscore,Xscore]
plt.bar(x_values,y_values,color=['red','orange','yellow','blue','pink','grey','black'])
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.title('Model Accuracy Comparison')
plt.show()
plt.tight_layout()

















